package executor

import (
	"context"
	"fmt"
	"math/rand"
	"sort"
	"sync"
	"time"

	"github.com/cznic/mathutil"
	"github.com/pingcap/errors"
	"github.com/pingcap/parser/model"
	"github.com/pingcap/tidb/domain"
	"github.com/pingcap/tidb/statistics"
	"github.com/pingcap/tidb/store/tikv"
	"github.com/pingcap/tidb/util/chunk"
	"github.com/pingcap/tidb/util/logutil"
	"go.uber.org/zap"
)

// step0 å®šä¹‰å…¨å±€è§¦å‘æ–¹æ³•ï¼Œç›¸å½“äº executor çš„ Next()ï¼Œæ–¹ä¾¿ä¹‹åä»£ç å¤ç”¨
// step1 å®šä¹‰ä¸€ä¸ªæ‰§è¡Œå™¨
// step2 å¤ç”¨ç°æœ‰çš„æ‰§è¡Œè®¡åˆ’ Task(plannercore/)ã€analyzeTaskã€analyzeResult
// step3 å®šä¹‰ä¸€ä¸ª worker å‡½æ•°å¯åŠ¨å¤šä¸ªçº¿ç¨‹æ‰§è¡Œ sampleing è¿‡ç¨‹
// AnalyzeSample çš„ open æš‚ä¸å®ç°

// è§¦å‘çš„å…¥å£ (å¯¹åº” AnalyzeExec çš„ Next()ï¼ŒåŒæ—¶æ”¾åœ¨é‚£é‡Œä½œä¸ºè§¦å‘)
func AnalyzeSample(e *AnalyzeExec, ctx context.Context, sampleSize uint64) error {
	// è·å–è¡¨ä¿¡æ¯
	// å®šä¹‰ TaskChã€ResultCh
	// build AnalyzeSampleExec(task,ä¸€ä¸ªtaskå¯¹åº”ä¸€ä¸ªexec)
	// å¹¶å‘å¯(n)ä¸ª worker do Sampling
	// res <-ResultCh (è¿”å›ä¸€æ•´ä¸ªè¡¨çš„ Result)
	// å¤„ç†æ•´åˆ res
	// æ›´æ–° cache (statsHandle.Update(GetInfoSchema(e.ctx)))
	// \---åœ¨ handle ä¸­æ·»åŠ ä¸€ä¸ªå‡½æ•°åªåš cache(--statistics.Table) çš„æ›´æ–°

	taskCh := make(chan *analyzeSampleTask, len(e.tasks))
	resultCh := make(chan analyzeSampleResult, len(e.tasks))
	for _, task := range e.tasks {
		buildAnalyzeSampleTask(e, task, taskCh, sampleSize)
	}

	concurrency, err := getBuildStatsConcurrency(e.ctx)
	if err != nil {
		return err
	}

	e.wg.Add(concurrency)
	for i := 0; i < concurrency; i++ {
		go analyzeSampleWorker(taskCh, resultCh, i == 0)
	}
	for _, task := range e.tasks {
		statistics.AddNewAnalyzeJob(task.job)
	}
	close(taskCh)

	// æ¥æ”¶ç»“æœ
	statsHandle := domain.GetDomain(e.ctx).StatsHandle()
	panicCnt := 0
	for panicCnt < concurrency {
		result, ok := <-resultCh
		if !ok {
			break
		}
		if result.Err != nil {
			err = result.Err
			if err == errAnalyzeWorkerPanic {
				panicCnt++
			} else {
				logutil.Logger(ctx).Error("analyze failed", zap.Error(err))
			}
			result.job.Finish(true)
			continue
		}
		// å¤„ç†ç»“æœ
		// tbl := statsHandle.GetTableStats()
	}
	return nil
}

type analyzeSampleTask struct {
	sampleExec *AnalyzeSampleExec
	job        *statistics.AnalyzeJob
}

type AnalyzeSampleExec struct {
	AnalyzeFastExec
	sampleSize uint64
}

type analyzeSampleResult struct {
	PhysicalTableID int64
	Sample          []*chunk.Column
	//Count           int64
	IsIndex int
	Err     error
	job     *statistics.AnalyzeJob
}

// build ä¸€ä¸ª AnalyzeSampleExecï¼Œå¯¹åº”äº executor/builder ä¸­çš„å·¥ä½œ
// å¹¶ä¸”å°†æ–°çš„ analyzeSampleTask è£…å…¥å…¶ä¸­
func buildAnalyzeSampleTask(e *AnalyzeExec, task *analyzeTask, taskCh chan *analyzeSampleTask, sampleSize uint64) {
	// å¯¹åº”æ¯ä¸ª A çš„ taskï¼Œå»ºç«‹ä¸€ä¸ª analyzeSampleTask
	// taskCh <-analyzeSampleTask

	var aspe AnalyzeSampleExec
	aspe.sampleSize = sampleSize
	aspe.ctx = e.ctx
	aspe.wg = &sync.WaitGroup{}

	// ä¸€ä¸ª idx å¯¹åº”ä¸€ä¸ªä»»åŠ¡ï¼Œä¸€ä¸ªè¡¨çš„æ‰€æœ‰ col å¯¹åº”ä¸€ä¸ªä»»åŠ¡
	switch task.taskType {
	case colTask:
		aspe.colsInfo = task.colExec.colsInfo
		aspe.concurrency = task.colExec.concurrency
		aspe.physicalTableID = task.colExec.physicalTableID
	case idxTask:
		aspe.idxsInfo = []*model.IndexInfo{task.idxExec.idxInfo}
		aspe.concurrency = task.idxExec.concurrency
	case fastTask:
		aspe.colsInfo = task.fastExec.colsInfo
		aspe.idxsInfo = task.fastExec.idxsInfo
		aspe.pkInfo = task.fastExec.pkInfo
		aspe.tblInfo = task.fastExec.tblInfo
	case pkIncrementalTask, idxIncrementalTask:
		fmt.Println("do nothing for Incremental")
	}

	taskCh <- &analyzeSampleTask{
		sampleExec: &aspe,
		job: &statistics.AnalyzeJob{
			DBName:        task.job.DBName,
			TableName:     task.job.TableName,
			PartitionName: task.job.PartitionName,
			JobInfo:       "sample analyze"},
	}
}

// analyzeWorker è´Ÿè´£å¤„ç† TaskCh ä¸­çš„æ‰€æœ‰ analyzeTask
func analyzeSampleWorker(taskCh <-chan *analyzeSampleTask, resultCh chan<- analyzeSampleResult, isCloseChanThread bool) {
	// ä» TaskCh ä¸­æ¥æ”¶ analyzeTask
	// ResultCh <-analyzeSample()
	for {
		task, ok := <-taskCh
		if !ok {
			break
		}
		task.job.Start()
		task.sampleExec.job = task.job
		for _, result := range analyzeSampleExec(task.sampleExec) {
			resultCh <- result
		}
	}
}

//---- ğŸµ----//
// å¤„ç†å•ä¸ªè¡¨çš„é‡‡æ ·ï¼Œè¿”å›æ‰€æœ‰åˆ—ä¸Šçš„ç»Ÿè®¡ä¿¡æ¯
// ç”±äºå¯¹äºä¸€ä¸ªè¡¨çš„é‡‡æ ·å¯èƒ½æ˜¯idxé‡‡æ ·å’Œcolmçš„é‡‡æ ·ï¼Œæ‰€ä»¥å¯èƒ½éœ€è¦è¿”å› 2 ä¸ª analyzeResult
func analyzeSampleExec(exec *AnalyzeSampleExec) []analyzeSampleResult {
	sample, err := exec.buildSample()
	if err != nil {
		return []analyzeSampleResult{{Err: err, job: exec.job}}
	}
	var results []analyzeSampleResult
	hasPKInfo := 0
	if exec.pkInfo != nil {
		hasPKInfo = 1
	}
	// å¤„ç†å­˜åœ¨ç´¢å¼•çš„æƒ…å†µ
	if len(exec.idxsInfo) > 0 {
		for i := hasPKInfo + len(exec.idxsInfo); i < len(sample); i++ {
			idxResult := analyzeSampleResult{
				PhysicalTableID: exec.physicalTableID,
				Sample:          []*chunk.Column{sample[i]},
				IsIndex:         1,
				job:             exec.job,
			}
			results = append(results, idxResult)
		}
	}
	// å¤„ç†åˆ—ä¸Šçš„é‡‡é›†
	colResult := analyzeSampleResult{
		PhysicalTableID: exec.physicalTableID,
		Sample:          sample[:hasPKInfo+len(exec.idxsInfo)],
		job:             exec.job,
	}
	results = append(results, colResult)
	return nil
}

// ä¸‹é¢æ˜¯ AnalyzeSampleExec éœ€è¦å®ç°çš„ä¸€äº›æ¥å£å‡½æ•° --> å¯¹æ‰€æœ‰åˆ—æˆ–å•ä¸ªç´¢å¼•ä¸Šçš„é‡‡æ ·å¤„ç†

// æ„å»ºé‡‡æ ·ï¼Œéœ€è¦å°† AnalyzeResult ä¸­ Sample ä¼ è¿›å»
func (e *AnalyzeSampleExec) buildSample() (sample []*chunk.Column, err error) {
	// æµ‹è¯•éœ€è¦æ‰€ä»¥é»˜è®¤å€¼ä¸º 1
	if RandSeed == 1 {
		e.randSeed = time.Now().UnixNano()
	} else {
		e.randSeed = RandSeed
	}
	rander := rand.New(rand.NewSource(e.randSeed))

	needRebuild, maxBuildTimes := true, 5
	regionErrorCounter := 0
	for counter := maxBuildTimes; needRebuild && counter > 0; counter-- {
		regionErrorCounter++
		needRebuild, err = e.buildSampTask()
		if err != nil {
			return nil, err
		}
	}

	if needRebuild {
		errMsg := "build Sample analyze task failed, exceed maxBuildTimes: %v"
		return nil, errors.Errorf(errMsg, maxBuildTimes)
	}

	defer e.job.Update(int64(e.rowCount))

	// å¦‚æœè¡¨çš„æ€»è¡Œæ•°å°äºæ ·æœ¬é‡çš„2å€,ç›´æ¥å…¨è¡¨æ‰«æ
	if e.rowCount < e.sampleSize*2 {
		for _, task := range e.sampTasks {
			e.scanTasks = append(e.scanTasks, task.Location)
		}
		e.sampTasks = e.sampTasks[:0]
		e.rowCount = 0
		return e.runTasks()
	}

	randPos := make([]uint64, 0, e.sampleSize+1)
	for i := 0; i < int(e.sampleSize); i++ {
		randPos = append(randPos, uint64(rander.Int63n(int64(e.rowCount))))
	}
	sort.Slice(randPos, func(i, j int) bool { return randPos[i] < randPos[j] })

	for _, task := range e.sampTasks {
		begin := sort.Search(len(randPos), func(i int) bool { return randPos[i] >= task.BeginOffset })
		end := sort.Search(len(randPos), func(i int) bool { return randPos[i] >= task.EndOffset })
		task.SampSize = uint64(end - begin)
	}
	return e.runTasks()
}

func (e *AnalyzeSampleExec) runTasks() ([]*chunk.Column, error) {
	errs := make([]error, e.concurrency)
	hasPKInfo := 0
	if e.pkInfo != nil {
		hasPKInfo = 1
	}

	length := len(e.colsInfo) + hasPKInfo + len(e.idxsInfo)
	e.collectors = make([]*statistics.SampleCollector, length)
	for i := range e.collectors {
		e.collectors[i] = &statistics.SampleCollector{
			MaxSampleSize: int64(e.sampleSize),
			Samples:       make([]*statistics.SampleItem, e.sampleSize),
		}
	}

	e.wg.Add(e.concurrency)
	bo := tikv.NewBackoffer(context.Background(), 500)
	for i := 0; i < e.concurrency; i++ {
		go e.handleSampTasks(bo, i, &errs[i])
	}
	e.wg.Wait()
	for _, err := range errs {
		if err != nil {
			return nil, err
		}
	}

	_, err := e.handleScanTasks(bo)
	// fastAnalyzeHistogramScanKeys.Observe(float64(scanKeysSize))
	if err != nil {
		return nil, err
	}

	stats := domain.GetDomain(e.ctx).StatsHandle()
	rowCount := int64(e.rowCount)
	if stats.Lease() > 0 {
		rowCount = mathutil.MinInt64(stats.GetTableStats(e.tblInfo).Count, rowCount)
	}

	// ç”Ÿæˆé‡‡æ ·ç»“æœ
	samples := make([]*chunk.Column, length)
	for i := 0; i < length; i++ {
		// ç”Ÿæˆæ”¶é›†å™¨å±æ€§
		collector := e.collectors[i]
		collector.Samples = collector.Samples[:e.sampCursor]
		sort.Slice(collector.Samples, func(i, j int) bool { return collector.Samples[i].RowID < collector.Samples[j].RowID })
		collector.CalcTotalSize()

		rowCount = mathutil.MaxInt64(rowCount, int64(len(collector.Samples)))
		collector.TotalSize *= rowCount / int64(len(collector.Samples))

		if i < hasPKInfo {
			samples[i], err = e.buildICSample(e.collectors[i])
		} else if i < hasPKInfo+len(e.colsInfo) {
			samples[i], err = e.buildICSample(e.collectors[i])
		} else {
			samples[i], err = e.buildICSample(e.collectors[i])
		}
		if err != nil {
			return nil, err
		}
	}
	return samples, nil
}

func (e *AnalyzeSampleExec) buildICSample(collector *statistics.SampleCollector) (*chunk.Column, error) {
	sc := e.ctx.GetSessionVars().StmtCtx
	sampleItems := collector.Samples
	err := statistics.SortSampleItems(sc, sampleItems)
	if err != nil {
		return nil, err
	}
	var sample *chunk.Column
	for i := 0; i < len(sampleItems); i++ {
		sample.AppendBytes(sampleItems[i].Value.GetBytes())
	}
	return sample, nil
}

// func (e *AnalyzeSampleExec) buildColumnSample(ID int64, collector *statistics.SampleCollector, tp *types.FieldType, rowCount int64) (*chunk.Column, error) {
// 	return nil, nil
// }
